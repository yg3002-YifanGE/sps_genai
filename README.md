# SPS GenAI - Assignments 1-5

**Columbia University ‚Äì APANPS5900 Applied Generative AI**  
**Author:** Yifan Ge (yg3002)  
**Repository:** https://github.com/yg3002-YifanGE/sps_genai

---

## üìã Project Overview

This project implements multiple generative AI models and integrates them into a unified FastAPI web service. The project covers assignments 1-5 of the course, progressively building a comprehensive generative AI system including fine-tuned large language models.

---

## üéØ Assignments Summary

### Assignment 1: Text Generation & Embeddings
- **Bigram text generation model**
- **SpaCy word embeddings** (en_core_web_md)
- **Word similarity search** using cosine similarity
- **FastAPI integration** with interactive documentation

**Key Endpoints:**
- `/generate` - Generate text using bigram probabilities
- `/embed` - Get word embedding vectors
- `/similar` - Find similar words based on embeddings

### Assignment 2: CNN Image Classification
- **CNN_A2 architecture** for image classification
- **CIFAR-10 dataset** (10 classes: airplane, car, bird, etc.)
- **Training and evaluation** with 64√ó64 RGB images
- **Model deployment** via API endpoint

**Key Features:**
- 2 Convolutional layers with MaxPooling
- 2 Fully connected layers
- Accuracy: ~70% on CIFAR-10 test set

**Key Endpoint:**
- `/predict_cifar10` - Upload image for classification

### Assignment 3: GAN for Image Generation
- **Generative Adversarial Network (GAN)** on MNIST
- **Generator:** Noise ‚Üí 28√ó28 grayscale images
- **Discriminator:** Binary classifier (real/fake)
- **BCE loss** with Adam optimizer

**Architecture:**
- Generator: FC ‚Üí ConvTranspose layers ‚Üí Tanh
- Discriminator: Conv layers ‚Üí FC ‚Üí Sigmoid
- Latent dimension: 100

**Key Endpoint:**
- `/gan/generate` - Generate handwritten digits

### Assignment 4: Energy-Based Model & Diffusion Model
- **Energy-Based Model (EBM)** on CIFAR-10
- **Diffusion Model** with UNet on CIFAR-10
- **Fine-grained gradient control** with `torch.autograd.grad()`
- **RGB image generation** (32√ó32√ó3)

**EBM Features:**
- Langevin dynamics for sampling low-energy states
- Contrastive divergence training
- Gradient descent on **input images** (not parameters)

**Diffusion Features:**
- UNet architecture with skip connections
- Offset cosine diffusion schedule
- Reverse diffusion for generation
- Predicts noise (not images directly)

**Key Endpoints:**
- `/ebm/generate` - Generate images using EBM
- `/diffusion/generate` - Generate images using Diffusion

### Assignment 5: Fine-tuned GPT-2 for Question Answering
- **Fine-tuned GPT-2** on SQuAD dataset (5,000 samples)
- **Custom response format**: Prefix + Answer + Suffix
- **API integration** with configurable generation parameters

**Quick Start:**
```bash
# Train model
cd assignment5
python train_gpt2_squad.py --epochs 1 --num_samples 5000 --batch_size 4

# Test model
python train_gpt2_squad.py --test_only

# Test API
python test_gpt2_api.py
```

**Key Endpoints:**
- `/gpt2/answer` - Single answer generation
- `/gpt2/answer/multiple` - Multiple diverse answers

üìÅ **See `assignment5/README.md` for complete documentation**

---

## üèóÔ∏è Project Structure

```
sps_genai/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application with all endpoints
‚îÇ   ‚îú‚îÄ‚îÄ bigram_model.py      # Text generation model
‚îÇ   ‚îî‚îÄ‚îÄ gpt2_qa.py           # Fine-tuned GPT-2 integration (Assignment 5)
‚îÇ
‚îú‚îÄ‚îÄ helper_lib/              # Reusable ML components
‚îÇ   ‚îú‚îÄ‚îÄ model.py             # CNN, VAE, GAN, EBM, Diffusion models
‚îÇ   ‚îú‚îÄ‚îÄ trainer.py           # Training functions
‚îÇ   ‚îú‚îÄ‚îÄ generator.py         # Sample generation utilities
‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py       # Dataset loaders
‚îÇ   ‚îî‚îÄ‚îÄ utils.py
‚îÇ
‚îú‚îÄ‚îÄ assignment2/             # CNN Image Classification
‚îÇ   ‚îî‚îÄ‚îÄ cnn_a2_cifar10.pt
‚îÇ
‚îú‚îÄ‚îÄ assignment4/             # EBM & Diffusion Models
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ train_ebm_cifar10.py
‚îÇ   ‚îú‚îÄ‚îÄ train_diffusion_cifar10.py
‚îÇ   ‚îî‚îÄ‚îÄ train_gan_offline.py
‚îÇ
‚îú‚îÄ‚îÄ assignment5/             # Fine-tuned GPT-2 Q&A
‚îÇ   ‚îú‚îÄ‚îÄ README.md            # Complete documentation
‚îÇ   ‚îú‚îÄ‚îÄ train_gpt2_squad.py  # Training script
‚îÇ   ‚îî‚îÄ‚îÄ test_gpt2_api.py     # API testing
‚îÇ
‚îú‚îÄ‚îÄ data/                    # Trained model weights
‚îÇ   ‚îú‚îÄ‚îÄ cifar-10-batches-py/
‚îÇ   ‚îú‚îÄ‚îÄ gan_G.pth
‚îÇ   ‚îú‚îÄ‚îÄ ebm_cifar10.pth
‚îÇ   ‚îî‚îÄ‚îÄ diffusion_cifar10.pth
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ gpt2_squad_finetuned/ # Fine-tuned GPT-2 (created by training)
‚îÇ
‚îú‚îÄ‚îÄ test_api.py              # General API testing
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md                # This file
```

---

## üöÄ Quick Start

### 1. Install Dependencies

**Using uv (recommended):**
```bash
uv sync
uv pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl
```

**Using pip:**
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_md
```

### 2. Train Models (if needed)

```bash
# Assignment 3: GAN
cd assignment4
python train_gan_offline.py

# Assignment 4: EBM & Diffusion
python train_ebm_cifar10.py
python train_diffusion_cifar10.py

# Assignment 5: Fine-tune GPT-2
cd ../assignment5
python train_gpt2_squad.py --epochs 1 --num_samples 5000 --batch_size 4
```

### 3. Run API Server

```bash
uvicorn app.main:app --port 8000 --reload
```

Access interactive documentation: **http://localhost:8000/docs**

### 4. Test API

```bash
# Test general endpoints
python test_api.py

# Test GPT-2 endpoints (Assignment 5)
cd assignment5
python test_gpt2_api.py
```

---

## üåê API Endpoints

| Endpoint | Method | Description | Assignment |
|----------|--------|-------------|-----------|
| `/` | GET | API status and endpoint list | - |
| `/generate` | POST | Text generation (bigram) | 1 |
| `/embed` | POST | Word embedding vector | 1 |
| `/similar` | POST | Find similar words | 1 |
| `/predict_cifar10` | POST | CIFAR-10 image classification | 2 |
| `/gan/generate` | GET | Generate MNIST digits | 3 |
| `/ebm/generate` | GET | Generate CIFAR-10 images (EBM) | 4 |
| `/diffusion/generate` | GET | Generate CIFAR-10 images (Diffusion) | 4 |
| `/gpt2/answer` | POST | Answer question with fine-tuned GPT-2 | 5 |
| `/gpt2/answer/multiple` | POST | Generate multiple diverse answers | 5 |

### Example Usage

**Text Generation:**
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{"start_word": "The", "length": 10}'
```

**EBM Generation:**
```bash
curl "http://localhost:8000/ebm/generate?num_samples=16&steps=256"
```

**Diffusion Generation:**
```bash
curl "http://localhost:8000/diffusion/generate?num_samples=16&diffusion_steps=50"
```

**GPT-2 Question Answering (Assignment 5):**
```bash
# Single answer
curl -X POST "http://localhost:8000/gpt2/answer" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is machine learning?",
    "max_length": 150,
    "temperature": 0.7
  }'

# Multiple diverse answers
curl -X POST "http://localhost:8000/gpt2/answer/multiple" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "What is artificial intelligence?",
    "num_responses": 3,
    "temperature": 0.8
  }'
```

---

## üê≥ Docker Deployment

### Build Image
```bash
docker build -t sps-genai:latest .
```

### Run Container
```bash
docker run -d -p 8000:8000 --name sps-genai sps-genai:latest
```

### Train GPT-2 Model in Container (Assignment 5)
```bash
# Quick test (1,000 samples, ~50 minutes)
docker exec -it sps-genai python assignment5/train_gpt2_squad.py --epochs 1 --num_samples 1000 --batch_size 4
```

Access at: **http://localhost:8000/docs**

üìñ **See `DOCKER_GUIDE.md` for complete Docker deployment guide**

---

## üîë Key Implementation Details

### Assignment 4: Fine-Grained Gradient Control

**EBM Sampling (Langevin Dynamics):**
```python
inp_imgs.requires_grad_(True)
energy = model(inp_imgs)
# Compute gradients w.r.t. INPUT (not model parameters)
grads, = torch.autograd.grad(energy, inp_imgs, grad_outputs=torch.ones_like(energy))
# Gradient descent on input to find low-energy states
inp_imgs = inp_imgs - step_size * grads
```

**Diffusion Training:**
```python
# Model predicts the NOISE, not the image
pred_noises = model(noisy_images, noise_rates ** 2)
loss = criterion(pred_noises, noises)  # Compare predicted vs true noise
```

---

## üìä Model Specifications

### CNN_A2 (Assignment 2)
- Input: 64√ó64√ó3 RGB images
- Architecture: Conv(3‚Üí16) ‚Üí Pool ‚Üí Conv(16‚Üí32) ‚Üí Pool ‚Üí FC(100) ‚Üí FC(10)
- Dataset: CIFAR-10
- Output: 10 class probabilities

### GAN (Assignment 3)
- Generator: Latent(100) ‚Üí FC ‚Üí ConvTranspose ‚Üí 28√ó28√ó1
- Discriminator: 28√ó28√ó1 ‚Üí Conv ‚Üí FC ‚Üí Sigmoid
- Dataset: MNIST
- Training: 10 epochs, BCE loss

### EBM (Assignment 4)
- Architecture: 4 Conv layers with Swish activation ‚Üí FC ‚Üí Scalar energy
- Input: 32√ó32√ó3 RGB (CIFAR-10)
- Training: Contrastive divergence, 10 epochs
- Sampling: Langevin dynamics (60 steps)

### Diffusion (Assignment 4)
- Architecture: UNet with skip connections
- Input: 32√ó32√ó3 RGB (CIFAR-10)
- Training: L1 loss on noise prediction, 5 epochs
- Sampling: Reverse diffusion (50 steps)

### Fine-tuned GPT-2 (Assignment 5)
- Architecture: GPT-2 (124M parameters, 12 layers, 768 hidden size)
- Dataset: SQuAD (Stanford Question Answering Dataset)
- Training: Causal language modeling, 3 epochs
- Format: Custom prefix/suffix for structured responses
- Generation: Autoregressive with temperature sampling

---

## ‚öôÔ∏è Dependencies

- Python >= 3.10
- torch >= 2.1.0
- torchvision >= 0.16.0
- fastapi[standard] >= 0.116.1
- pydantic >= 2.0.0
- spacy >= 3.7.0
- transformers >= 4.35.0 (Assignment 5)
- datasets >= 2.14.0 (Assignment 5)
- accelerate >= 0.24.0 (Assignment 5)
- numpy >= 1.24.0
- pillow >= 10.0.0
- matplotlib >= 3.7.0
- tqdm >= 4.66.0

---

## üìù Training Details

### EBM Training
- Epochs: 10
- Learning rate: 1e-4
- Langevin steps: 60
- Regularization Œ±: 0.1
- Time: ~1 hour (CPU)

### Diffusion Training
- Epochs: 5
- Learning rate: 1e-3
- Diffusion schedule: Offset cosine
- Time: ~20 minutes (CPU)

### GAN Training
- Epochs: 10
- Learning rate: 2e-4
- Optimizer: Adam (Œ≤1=0.5, Œ≤2=0.999)
- Time: ~15 minutes (CPU)

### GPT-2 Fine-tuning (Assignment 5)
- Base model: openai-community/gpt2 (124M params)
- **Dataset: SQuAD (5,000 samples from 87k total)** ‚≠ê
- Epochs: 1
- Learning rate: 5e-5
- Batch size: 4
- Max sequence length: 512 tokens
- Time: ~4-5 hours (CPU)
- Format: Custom prefix/suffix wrapping

**Dataset Size Rationale:**
We use 5,000 samples (instead of the full 87k) because:
1. **Task-appropriate**: This is fine-tuning for format learning, not training from scratch
2. **Computationally efficient**: Full dataset would take 80+ hours on CPU
3. **Sufficient for format**: 5,000 examples adequately teach the response template
4. **Assignment-compliant**: Meets all requirements while being practical

**Training Command:**
```bash
# Recommended configuration (used for this submission)
python train_gpt2_squad_simple.py --epochs 1 --num_samples 5000 --batch_size 4

# Alternative: Quick test (1,000 samples, ~50 minutes)
python train_gpt2_squad_simple.py --epochs 1 --num_samples 1000 --batch_size 4

# Alternative: Full dataset (not recommended - 80+ hours)
python train_gpt2_squad_simple.py --epochs 1 --batch_size 4

# Test existing model
python train_gpt2_squad_simple.py --test_only
```

---

## ‚úÖ Assignment 5: Fine-tuned LLM Features

**What's New in Assignment 5:**
- ‚úÖ Fine-tuned GPT-2 model on SQuAD dataset
- ‚úÖ Custom response format with prefix/suffix
- ‚úÖ Question answering API endpoints
- ‚úÖ Multiple response generation
- ‚úÖ Configurable generation parameters (temperature, max_length, top_p)
- ‚úÖ Lazy model loading for efficient API startup
- ‚úÖ Comprehensive testing script
- ‚úÖ Full documentation and examples

**Custom Response Format:**
```
That is a great question. [Question] [Answer] Let me know if you have any other questions.
```

---

## ‚úÖ Previous Assignment Issues - Fixed

- ‚úÖ All dependencies now in `pyproject.toml`
- ‚úÖ Docker configured to use port 8000 (not 80)
- ‚úÖ `helper_lib/` and `data/` folders included in Docker
- ‚úÖ Uvicorn starts without manual intervention
- ‚úÖ No missing dependencies
- ‚úÖ API launches successfully

---

## üß™ Testing

Run the test script to verify all endpoints:
```bash
python test_api.py
```

This will:
- Test all API endpoints
- Generate sample images
- Save results to local files
- Display success/failure status

---

## üìö References

- Course: APANPS5900 Applied Generative AI
- Modules: 1-11 (Text Generation to Reinforcement Learning)
- PyTorch Documentation
- FastAPI Documentation
- HuggingFace Transformers Documentation
- SQuAD Dataset: https://huggingface.co/datasets/rajpurkar/squad

---

## üìÑ License

This project is for educational purposes as part of Columbia University coursework.

---

**Status:** ‚úÖ All assignments (1-5) complete and deployed

---

## üéì Assignment 5 Specific Notes

### Fine-tuning Process
1. **Data Preparation**: SQuAD dataset is automatically downloaded from HuggingFace
2. **Format Transformation**: Q&A pairs are wrapped with custom prefix/suffix
3. **Tokenization**: GPT-2 tokenizer with padding and truncation
4. **Training**: Causal language modeling with teacher forcing
5. **Saving**: Model and tokenizer saved to `models/gpt2_squad_finetuned/`

### API Integration
- **Lazy Loading**: Model loads only when first API call is made
- **Device Detection**: Automatically uses GPU if available
- **Error Handling**: Comprehensive error messages for missing models
- **Validation**: Parameter validation for generation settings

### Testing
```bash
# Run comprehensive tests
python test_gpt2_api.py
```

Tests include:
- API health check
- Single answer generation with multiple questions
- Multiple answer generation
- Parameter validation
- Sample output generation for documentation

### Performance Tips
- **GPU**: Use CUDA-enabled GPU for faster inference (3-5x speedup)
- **Batch Size**: Adjust based on available memory
- **Temperature**: Lower (0.5-0.7) for focused answers, higher (0.8-1.0) for creativity
- **Max Length**: Balance between completeness and inference time

---
